{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Boston Housing Prices_Eager_Execution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Load Tensorflow and enable eager execution"
      ],
      "metadata": {
        "id": "2OjFZ7vNR8hR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xPjg843PRvm1",
        "outputId": "217df6c4-dfab-4d14-ff20-e81bef8dff46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "#check tf version\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data\n"
      ],
      "metadata": {
        "id": "VLtZcxJQSSPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_x,train_y),(_,_)=tf.keras.datasets.boston_housing.load_data(test_split=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAgYSG9pSREu",
        "outputId": "6ff154c7-4a95-4bc7-dae3-6bd661063b6d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n",
            "65536/57026 [==================================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKxllg-bSfKq",
        "outputId": "210a9cdc-b84f-435e-f853-e57a3cd5b290"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyYuxyH2SiSw",
        "outputId": "e7bb1dbc-df5a-44f7-8175-5f2aadd5f155"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506,)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52PSvIFESkP1",
        "outputId": "5b9ccd1d-4102-4c29-cab8-b58f0d5593ce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x=train_x.astype('float32')\n",
        "train_y=train_y.astype('float32')"
      ],
      "metadata": {
        "id": "IK4YE6WOSnxZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Model"
      ],
      "metadata": {
        "id": "boc6ltsKTJor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we are initializing weights and bias with zero\n",
        "w=tf.zeros(shape=(13,1))\n",
        "b=tf.zeros(shape=(1))"
      ],
      "metadata": {
        "id": "DHxDlD0ATBos"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function to calculate  prediction\n"
      ],
      "metadata": {
        "id": "F5sbynSAT1WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(x,w,b):\n",
        "  xw_matmul=tf.matmul(x,w)\n",
        "  y=tf.add(xw_matmul,b)\n",
        "  return y"
      ],
      "metadata": {
        "id": "L1eMQ-8KT9Op"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to calculate loss(mean squared error)"
      ],
      "metadata": {
        "id": "CvFLx0JxUMiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(y_actual,y_predicted):\n",
        "  diff=y_actual-y_predicted\n",
        "  sqr=tf.square(diff)\n",
        "  avg=tf.reduce_mean(sqr)\n",
        "  return avg"
      ],
      "metadata": {
        "id": "aSVC_jFuUQkB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to train the Model\n",
        "  1.Record all the mathematical steps to calculate Loss. We will record the steps using Gradient Tape\n",
        "  2.Calculate Gradients of loss w.r.t weights and bias\n",
        "  3.Update weights and bias based on gradients and learning rate"
      ],
      "metadata": {
        "id": "l1Hrk-DpU6HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(x,y_actual,w,b,learning_rate=0.01):\n",
        "\n",
        "  #record mathematical operations on 'tape' to calculate loss\n",
        "  with tf.GradientTape() as t:\n",
        "    t.watch([w,b])\n",
        "    current_prediction=prediction(x,w,b)\n",
        "    current_loss=loss(y_actual,current_prediction)\n",
        "  #Calculate gradiants for loss with respect to wieghts and bias\n",
        "  dw,db=t.gradient(current_loss,[w,b])\n",
        "\n",
        "  #update weights and bias\n",
        "  w=w-learning_rate*dw\n",
        "  b=b-learning_rate*db\n",
        "  return w,b"
      ],
      "metadata": {
        "id": "7xCMGDaKVeCv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start Training"
      ],
      "metadata": {
        "id": "SwORiHUPXPdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train for 100 steps\n",
        "for i in range(100):\n",
        "  w,b=train(train_x,train_y,w,b,learning_rate=0.01)\n",
        "  print(\"Current loss on iterations \",i,\" is :\",loss(train_y,prediction(train_x,w,b)).numpy())\n",
        "               "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjP0k8YLXQkM",
        "outputId": "13fc6bf8-5475-4204-b24c-812e5e25bee7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current loss on iterations  0  is : 19006894000.0\n",
            "Current loss on iterations  1  is : 7.446022e+17\n",
            "Current loss on iterations  2  is : 2.9173267e+25\n",
            "Current loss on iterations  3  is : 1.1429995e+33\n",
            "Current loss on iterations  4  is : inf\n",
            "Current loss on iterations  5  is : inf\n",
            "Current loss on iterations  6  is : inf\n",
            "Current loss on iterations  7  is : inf\n",
            "Current loss on iterations  8  is : inf\n",
            "Current loss on iterations  9  is : inf\n",
            "Current loss on iterations  10  is : nan\n",
            "Current loss on iterations  11  is : nan\n",
            "Current loss on iterations  12  is : nan\n",
            "Current loss on iterations  13  is : nan\n",
            "Current loss on iterations  14  is : nan\n",
            "Current loss on iterations  15  is : nan\n",
            "Current loss on iterations  16  is : nan\n",
            "Current loss on iterations  17  is : nan\n",
            "Current loss on iterations  18  is : nan\n",
            "Current loss on iterations  19  is : nan\n",
            "Current loss on iterations  20  is : nan\n",
            "Current loss on iterations  21  is : nan\n",
            "Current loss on iterations  22  is : nan\n",
            "Current loss on iterations  23  is : nan\n",
            "Current loss on iterations  24  is : nan\n",
            "Current loss on iterations  25  is : nan\n",
            "Current loss on iterations  26  is : nan\n",
            "Current loss on iterations  27  is : nan\n",
            "Current loss on iterations  28  is : nan\n",
            "Current loss on iterations  29  is : nan\n",
            "Current loss on iterations  30  is : nan\n",
            "Current loss on iterations  31  is : nan\n",
            "Current loss on iterations  32  is : nan\n",
            "Current loss on iterations  33  is : nan\n",
            "Current loss on iterations  34  is : nan\n",
            "Current loss on iterations  35  is : nan\n",
            "Current loss on iterations  36  is : nan\n",
            "Current loss on iterations  37  is : nan\n",
            "Current loss on iterations  38  is : nan\n",
            "Current loss on iterations  39  is : nan\n",
            "Current loss on iterations  40  is : nan\n",
            "Current loss on iterations  41  is : nan\n",
            "Current loss on iterations  42  is : nan\n",
            "Current loss on iterations  43  is : nan\n",
            "Current loss on iterations  44  is : nan\n",
            "Current loss on iterations  45  is : nan\n",
            "Current loss on iterations  46  is : nan\n",
            "Current loss on iterations  47  is : nan\n",
            "Current loss on iterations  48  is : nan\n",
            "Current loss on iterations  49  is : nan\n",
            "Current loss on iterations  50  is : nan\n",
            "Current loss on iterations  51  is : nan\n",
            "Current loss on iterations  52  is : nan\n",
            "Current loss on iterations  53  is : nan\n",
            "Current loss on iterations  54  is : nan\n",
            "Current loss on iterations  55  is : nan\n",
            "Current loss on iterations  56  is : nan\n",
            "Current loss on iterations  57  is : nan\n",
            "Current loss on iterations  58  is : nan\n",
            "Current loss on iterations  59  is : nan\n",
            "Current loss on iterations  60  is : nan\n",
            "Current loss on iterations  61  is : nan\n",
            "Current loss on iterations  62  is : nan\n",
            "Current loss on iterations  63  is : nan\n",
            "Current loss on iterations  64  is : nan\n",
            "Current loss on iterations  65  is : nan\n",
            "Current loss on iterations  66  is : nan\n",
            "Current loss on iterations  67  is : nan\n",
            "Current loss on iterations  68  is : nan\n",
            "Current loss on iterations  69  is : nan\n",
            "Current loss on iterations  70  is : nan\n",
            "Current loss on iterations  71  is : nan\n",
            "Current loss on iterations  72  is : nan\n",
            "Current loss on iterations  73  is : nan\n",
            "Current loss on iterations  74  is : nan\n",
            "Current loss on iterations  75  is : nan\n",
            "Current loss on iterations  76  is : nan\n",
            "Current loss on iterations  77  is : nan\n",
            "Current loss on iterations  78  is : nan\n",
            "Current loss on iterations  79  is : nan\n",
            "Current loss on iterations  80  is : nan\n",
            "Current loss on iterations  81  is : nan\n",
            "Current loss on iterations  82  is : nan\n",
            "Current loss on iterations  83  is : nan\n",
            "Current loss on iterations  84  is : nan\n",
            "Current loss on iterations  85  is : nan\n",
            "Current loss on iterations  86  is : nan\n",
            "Current loss on iterations  87  is : nan\n",
            "Current loss on iterations  88  is : nan\n",
            "Current loss on iterations  89  is : nan\n",
            "Current loss on iterations  90  is : nan\n",
            "Current loss on iterations  91  is : nan\n",
            "Current loss on iterations  92  is : nan\n",
            "Current loss on iterations  93  is : nan\n",
            "Current loss on iterations  94  is : nan\n",
            "Current loss on iterations  95  is : nan\n",
            "Current loss on iterations  96  is : nan\n",
            "Current loss on iterations  97  is : nan\n",
            "Current loss on iterations  98  is : nan\n",
            "Current loss on iterations  99  is : nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check Weights and Bias\n",
        "print('Weights:\\n',w.numpy())\n",
        "print('Bias: \\n',b.numpy())"
      ],
      "metadata": {
        "id": "Z60eR06nXxFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe0b809-9c0d-4842-bec7-b182634df0a2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights:\n",
            " [[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Bias: \n",
            " [nan]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w51J2Ct-OcX0",
        "outputId": "f271f796-77de-4fe3-bf98-060f8b9f0324"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1.23247,   0.     ,   8.14   ,   0.     ,   0.538  ,   6.142  ,\n",
              "        91.7    ,   3.9769 ,   4.     , 307.     ,  21.     , 396.9    ,\n",
              "        18.72   ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to normalize data"
      ],
      "metadata": {
        "id": "Rm6Cn6L6Pq_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Normalizer"
      ],
      "metadata": {
        "id": "3L-FKHD3Ogql"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer=Normalizer()\n",
        "train_x=transformer.fit_transform(train_x)\n",
        "train_x[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR-JZrLuP0ha",
        "outputId": "f7bacbc5-e8bf-45ad-8c3d-b416c60f2dd6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.0024119 , 0.        , 0.01592969, 0.        , 0.00105285,\n",
              "       0.01201967, 0.17945357, 0.00778265, 0.00782785, 0.6007879 ,\n",
              "       0.04109624, 0.7767189 , 0.03663436], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After normalization training model"
      ],
      "metadata": {
        "id": "ztmJ2xY0Q3rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train for 100 steps\n",
        "for i in range(100):\n",
        "  w,b=train(train_x,train_y,w,b,learning_rate=0.01)\n",
        "  print(\"Current loss on iterations \",i,\" is :\",loss(train_y,prediction(train_x,w,b)).numpy())\n",
        "               "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIahtibsQ5y_",
        "outputId": "0023cd8f-4eb9-4a57-9a08-fc498cd5476a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current loss on iterations  0  is : 84.948715\n",
            "Current loss on iterations  1  is : 84.93481\n",
            "Current loss on iterations  2  is : 84.92191\n",
            "Current loss on iterations  3  is : 84.909966\n",
            "Current loss on iterations  4  is : 84.8989\n",
            "Current loss on iterations  5  is : 84.88866\n",
            "Current loss on iterations  6  is : 84.87916\n",
            "Current loss on iterations  7  is : 84.870346\n",
            "Current loss on iterations  8  is : 84.862175\n",
            "Current loss on iterations  9  is : 84.85461\n",
            "Current loss on iterations  10  is : 84.84757\n",
            "Current loss on iterations  11  is : 84.84106\n",
            "Current loss on iterations  12  is : 84.83501\n",
            "Current loss on iterations  13  is : 84.829384\n",
            "Current loss on iterations  14  is : 84.824165\n",
            "Current loss on iterations  15  is : 84.81932\n",
            "Current loss on iterations  16  is : 84.814804\n",
            "Current loss on iterations  17  is : 84.81062\n",
            "Current loss on iterations  18  is : 84.806725\n",
            "Current loss on iterations  19  is : 84.80309\n",
            "Current loss on iterations  20  is : 84.799706\n",
            "Current loss on iterations  21  is : 84.79656\n",
            "Current loss on iterations  22  is : 84.793625\n",
            "Current loss on iterations  23  is : 84.790886\n",
            "Current loss on iterations  24  is : 84.78834\n",
            "Current loss on iterations  25  is : 84.78595\n",
            "Current loss on iterations  26  is : 84.783714\n",
            "Current loss on iterations  27  is : 84.78163\n",
            "Current loss on iterations  28  is : 84.779686\n",
            "Current loss on iterations  29  is : 84.77786\n",
            "Current loss on iterations  30  is : 84.77614\n",
            "Current loss on iterations  31  is : 84.77453\n",
            "Current loss on iterations  32  is : 84.77302\n",
            "Current loss on iterations  33  is : 84.7716\n",
            "Current loss on iterations  34  is : 84.77026\n",
            "Current loss on iterations  35  is : 84.769005\n",
            "Current loss on iterations  36  is : 84.767815\n",
            "Current loss on iterations  37  is : 84.766685\n",
            "Current loss on iterations  38  is : 84.765625\n",
            "Current loss on iterations  39  is : 84.764626\n",
            "Current loss on iterations  40  is : 84.76368\n",
            "Current loss on iterations  41  is : 84.76278\n",
            "Current loss on iterations  42  is : 84.76192\n",
            "Current loss on iterations  43  is : 84.76109\n",
            "Current loss on iterations  44  is : 84.760315\n",
            "Current loss on iterations  45  is : 84.75958\n",
            "Current loss on iterations  46  is : 84.75887\n",
            "Current loss on iterations  47  is : 84.758194\n",
            "Current loss on iterations  48  is : 84.75754\n",
            "Current loss on iterations  49  is : 84.75691\n",
            "Current loss on iterations  50  is : 84.75632\n",
            "Current loss on iterations  51  is : 84.75574\n",
            "Current loss on iterations  52  is : 84.75517\n",
            "Current loss on iterations  53  is : 84.75464\n",
            "Current loss on iterations  54  is : 84.75413\n",
            "Current loss on iterations  55  is : 84.753624\n",
            "Current loss on iterations  56  is : 84.75313\n",
            "Current loss on iterations  57  is : 84.75266\n",
            "Current loss on iterations  58  is : 84.7522\n",
            "Current loss on iterations  59  is : 84.751755\n",
            "Current loss on iterations  60  is : 84.75132\n",
            "Current loss on iterations  61  is : 84.750885\n",
            "Current loss on iterations  62  is : 84.75047\n",
            "Current loss on iterations  63  is : 84.75007\n",
            "Current loss on iterations  64  is : 84.74966\n",
            "Current loss on iterations  65  is : 84.749275\n",
            "Current loss on iterations  66  is : 84.74889\n",
            "Current loss on iterations  67  is : 84.74852\n",
            "Current loss on iterations  68  is : 84.748146\n",
            "Current loss on iterations  69  is : 84.74779\n",
            "Current loss on iterations  70  is : 84.74743\n",
            "Current loss on iterations  71  is : 84.747086\n",
            "Current loss on iterations  72  is : 84.74672\n",
            "Current loss on iterations  73  is : 84.74639\n",
            "Current loss on iterations  74  is : 84.74605\n",
            "Current loss on iterations  75  is : 84.74571\n",
            "Current loss on iterations  76  is : 84.74538\n",
            "Current loss on iterations  77  is : 84.74505\n",
            "Current loss on iterations  78  is : 84.74472\n",
            "Current loss on iterations  79  is : 84.74441\n",
            "Current loss on iterations  80  is : 84.74409\n",
            "Current loss on iterations  81  is : 84.743774\n",
            "Current loss on iterations  82  is : 84.74347\n",
            "Current loss on iterations  83  is : 84.74316\n",
            "Current loss on iterations  84  is : 84.74285\n",
            "Current loss on iterations  85  is : 84.742546\n",
            "Current loss on iterations  86  is : 84.74224\n",
            "Current loss on iterations  87  is : 84.74194\n",
            "Current loss on iterations  88  is : 84.74163\n",
            "Current loss on iterations  89  is : 84.74134\n",
            "Current loss on iterations  90  is : 84.74104\n",
            "Current loss on iterations  91  is : 84.74075\n",
            "Current loss on iterations  92  is : 84.740456\n",
            "Current loss on iterations  93  is : 84.74018\n",
            "Current loss on iterations  94  is : 84.73988\n",
            "Current loss on iterations  95  is : 84.73959\n",
            "Current loss on iterations  96  is : 84.7393\n",
            "Current loss on iterations  97  is : 84.73902\n",
            "Current loss on iterations  98  is : 84.73873\n",
            "Current loss on iterations  99  is : 84.73845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check weights and bias\n",
        "print(\"weights :\\n\",w.numpy())\n",
        "print('Bias :\\n',b.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxXMwygGRl8K",
        "outputId": "d29990ff-e6ee-415a-a2a4-4bac6eacc527"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights :\n",
            " [[6.9680542e-02]\n",
            " [2.6091012e-01]\n",
            " [2.2493085e-01]\n",
            " [1.4710182e-03]\n",
            " [1.1628192e-02]\n",
            " [1.3432980e-01]\n",
            " [1.4229288e+00]\n",
            " [8.3185226e-02]\n",
            " [1.8633363e-01]\n",
            " [8.2406321e+00]\n",
            " [3.9016062e-01]\n",
            " [7.4122663e+00]\n",
            " [2.6206842e-01]]\n",
            "Bias :\n",
            " [11.732036]\n"
          ]
        }
      ]
    }
  ]
}